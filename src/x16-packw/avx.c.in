// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.


#include <string.h>
$assert NR == 8
$assert KUNROLL == 16

#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include <immintrin.h>

#include <xnnpack/packw.h>
$if PREFETCH:
  #include <xnnpack/prefetch.h>

uint16_t buf[${KUNROLL}];

$def LOAD_TO_REGISTER(REGISTER, ROW, SIZE):
    memset(buf, 0, ${KUNROLL} * 2);
    memcpy(buf, ${ROW}, ${SIZE} * 2);
    __m256i ${REGISTER} = _mm256_loadu_si256((const __m256i*)buf);
    ${ROW} += ${SIZE};

$def LOAD_ROWS_TO_REGISTERS(IS_N_REMAINDER, IS_K_REMAINDER):
  $for N in range(0, NR-1):
    $if IS_N_REMAINDER and IS_K_REMAINDER:
      $LOAD_TO_REGISTER('v' + str(N), 'row' + str(N), 'k')
    $else:
      $if IS_K_REMAINDER:
         $LOAD_TO_REGISTER('v' + str(N), 'row' + str(N), 'k')
         row${N} += k;
      $else:
         __m256i v${N} = _mm256_loadu_si256((const __m256i*)row${N});
         row${N} += 16;
  $N = NR-1
  $if not IS_K_REMAINDER and not IS_N_REMAINDER:
    __m256i v${N} = _mm256_loadu_si256((const __m256i*)row${N});
    row${N} += 16;
  $elif IS_N_REMAINDER:
    __m256i v${N} = _mm256_set1_epi32(0);
  $elif IS_K_REMAINDER:
    $LOAD_TO_REGISTER('v' + str(N), 'row' + str(N), 'k')

$def TRANSPOSE():
    // Interleave 13-bit lanes
    $for FIRST in range(0, NR, 2):
      $SECOND = FIRST + 1
      __m256i vt${FIRST} = _mm256_unpacklo_epi16(v${FIRST}, v${SECOND});
      __m256i vt${SECOND} = _mm256_unpackhi_epi16(v${FIRST}, v${SECOND});

    // Interleave 32-bit lanes
    $OUT_INDEX = 0
    $for FIRST in [0, 1, 4, 5]:
        $SECOND = FIRST + 2
        v${OUT_INDEX} = _mm256_unpacklo_epi32(vt${FIRST}, vt${SECOND});
        v${OUT_INDEX + 1} = _mm256_unpackhi_epi32(vt${FIRST}, vt${SECOND});
        $OUT_INDEX += 2

    // Interleave 64-bit lanes
    $OUT_INDEX = 0
    $for FIRST in range(NR//2):
        $SECOND = FIRST + 4
        vt${OUT_INDEX} = _mm256_unpacklo_epi64(v${FIRST}, v${SECOND});
        vt${OUT_INDEX + 1} = _mm256_unpackhi_epi64(v${FIRST}, v${SECOND});
        $OUT_INDEX += 2

    $for PAIR_INDEX in range(NR//2):
        $FIRST = PAIR_INDEX * 2
        $SECOND = FIRST + 1
        v${FIRST} = _mm256_permute2f128_si256(vt${FIRST}, vt${SECOND}, 0x20);
        v${SECOND} = _mm256_permute2f128_si256(vt${FIRST}, vt${SECOND}, 0x31);

$def STORE_REGISTERS_FULLY():
    $for N in range(NR):
       _mm256_storeu_si256((__m256i*) packed_weights, v${ORDER[N]});
       packed_weights += 16;

$def STORE_REGISTERS_WITH_K_REMAINDER():
  $for N in range(NR):
      if (${(N + 1) * 2} <= k) {
          _mm256_storeu_si256((__m256i*) packed_weights, v${ORDER[N]});
          packed_weights += 16;
      }
      if (${N * 2 + 1} == k) {
          __m128i vtlow = _mm256_extracti128_si256(v${ORDER[N]}, 0);
          _mm_storeu_si128((__m128i*) packed_weights, vtlow);
          packed_weights += 8;
      }

void xnn_x16_packw_gemm_goi_ukernel_x${NR}__avx2_x${KUNROLL}(
  size_t g,
  size_t nc,
  size_t kc,
  size_t nr,
  size_t kr,
  size_t sr,
  const uint16_t* weights,
  const uint16_t* bias,
  uint16_t* packed_weights,
  size_t extra_bytes,
  const void* params)
{

  assert(g != 0);
  assert(nc != 0);
  assert(kc != 0);
  assert(nr == ${NR});   // This kernel is for NR=${NR}
  assert(kr == 1);
  assert(sr == 1);
  assert(weights != NULL);
  assert(packed_weights != NULL);


  do {
    const uint16_t* w0 = weights;
    size_t n = nc;
    for (; n >= ${NR}; n -= ${NR}) {
      $if NR == 8:
        $PREFIX = ""
        $TYPE_SIZE = "128"
      $else:
        $PREFIX = "256"
        $TYPE_SIZE = "256"
      $TYPE = "__m" + TYPE_SIZE + "i"
      ${TYPE} vtmp;
      if XNN_LIKELY(bias != NULL) {
        vtmp = _mm${PREFIX}_loadu_si${TYPE_SIZE}((const ${TYPE}*) bias);
        bias += ${NR};
      } else {
        vtmp = _mm${PREFIX}_set1_epi32(0);
      }
      _mm${PREFIX}_storeu_si${TYPE_SIZE}((${TYPE}*) packed_weights, vtmp);
      packed_weights += ${NR};

      $for N in range(0, NR):
        const uint16_t* row${N} = w0 + ${N} * kc;
      $ORDER = [0, 2, 4, 6, 1, 3, 5, 7]
      size_t k = kc;
      for (; k >= 16; k -= 16) {
        $LOAD_ROWS_TO_REGISTERS(False, False)
        $TRANSPOSE()
        $STORE_REGISTERS_FULLY()
      }
      // KC remainder
      $LOAD_ROWS_TO_REGISTERS(False, True)
      $TRANSPOSE()
      $STORE_REGISTERS_WITH_K_REMAINDER()
      packed_weights = (uint16_t*) ((uintptr_t) packed_weights + extra_bytes);
      w0 = row${NR-1};
    }


    // NC remainder (1..${NR-1})
    if XNN_UNLIKELY(n != 0) {
      assert(n >= 1);
      assert(n <= ${NR-1});
      if XNN_LIKELY(bias != NULL) {
        memcpy(packed_weights, bias, n * 2);
        bias += n;
      } else {
        memset(packed_weights, 0, ${NR*2});
      }
      packed_weights += ${NR};
      // NR remainder has less than ${NR} rows so last row is not loaded
      const uint16_t* row0 = w0;
      $for N in range(1, NR-1):
        const uint16_t* row${N} = row${N-1} + kc;
        $if N % 2 == 0:
          if XNN_UNPREDICTABLE(n <= ${N}) {
            row${N} = row${N-1};
          }
        $else:
          if XNN_UNPREDICTABLE(n < ${N+1}) {
            row${N} = row${N-1};
          }

      size_t k = kc;
      for (; k >= ${KUNROLL}; k -= ${KUNROLL}) {
        $LOAD_ROWS_TO_REGISTERS(True, False)
        $TRANSPOSE()
        $STORE_REGISTERS_FULLY()
      }

      // KC and NC remainder
      $LOAD_ROWS_TO_REGISTERS(True, True)
      $TRANSPOSE()
      $STORE_REGISTERS_WITH_K_REMAINDER()
      packed_weights = (uint16_t*) ((uintptr_t) packed_weights + extra_bytes);
    }
    weights += nc * kc;
  } while (--g != 0);
}
